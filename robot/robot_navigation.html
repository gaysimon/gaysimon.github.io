<!DOCTYPE html>
<html >
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<link rel="stylesheet" href="/style1.css" type="text/css" />
		<title>Simon GAY</title>
	</head>

	<body>
	
		<header>
			<div id="banniere_image"> </div>
		</header>
	
		<div class="main">
			<nav>
                	<a href="/index.html">Accueil</a><br />
			<a href="/recherches.html">Recherches</a><br />
			<a href="/postdoc.html">Mon PostDoc</a><br />
			<a href="/these.html">Ma These</a><br />
			<a href="/publi.html">Publications</a><br />
			<a href="/robot.html">Les Robots</a><br />
			<a href="/software.html">logiciels</a><br />
			<br />
			<a href="/robot/robot_navigation.html"><img src="/img/fr.png" alt="fr" /> </a>
			<a href="/robot/robot_navigation_en.html"><img src="/img/en.png" alt="en" /> </a>
			</nav>

			<section class="subsection">
				<p>
					<center style="text-align: center;font-size: xx-large;">Plateforme robotique omni-directionnelle</center><br />
					<br />
				</p>

				<figure style="text-align:center">
					<img src="/projects/robot.jpg" alt="robotic platform" width="600" />
					<figcaption>Une plateforme omni-directionnelle pour étudier des modèles de navigation autonome.</figcaption>
				</figure>
				
				<br/>
				<br/>
				<p>
					Ce robot a été conçu pour tester et valider nos <a href="https://gaysimon.github.io/postdoc/navig2.html">modèles de navigation bio-inspirés</a> en environnement réel. Le choix de la plateforme a été dictée par différentes contraintes : déplacements omni-directionnels pour simuler les déplacements d'une personne, abordable pour pouvoir s'équiper d'une flottille de robot, et d'une taille suffisante pour pouvoir être équipé d'un nano-ordinateur et d'une caméra binoculaire. Notre choix s'est porté sur la plateforme Mecanuum Wheel produit par Osoyoo. Cette plateforme a ensuite été équipée d'un étage supplémentaire pour accueillir de nouveaux composants.
				</p>
			
			</section>
			
			
			<section class="subsection">
				<section class="listsection">

					<p>
						<center style="text-align: center;font-size: xx-large;">La plateforme Mecanum Wheel</center><br /><br />
					</p>

					<p>
						La plateforme <a href=https://osoyoo.com/2022/07/05/v2-metal-chassis-mecanum-wheel-robotic-for-arduino-mega2560-introduction-model-2021006600/>Mecanuum Wheel de Osoyoo</a> constitue une base intéressante pour notre plateforme de test : elle est relativement simple à assembler et propose des déplacements omni-directionnel. Son architecture, reposant sur des composants répandus (Arduino et carte de contrôle moteur L293), facilite l'interface avec des composants additionnels.
					</p>


					<figure style="text-align:center">
						<img src="/robot/mecanuum_platform.jpg" alt="mecanuum platform" width="400" />
						<figcaption>La plateforme Mecanuum Wheel assemblée. L'Arduino est équipé d'un shield Wifi/Bluetooth. Le kit comprend également un sonar monté sur un servomoteur ainsi qu'un capteur de ligne au sol. </figcaption>
					</figure>

					<p>
						La plateforme est équipée de quatre roues dite "mecanum", c'est à dire dotées de petites roulettes internes. Contrairement aux roues omni-directionnelles (ou roues holonomes), dont les roulettes sont perpendiculaires à l'axe de la roue, les roues mecanum ont des roulettes inclinées de 45° par rapport à l'axe. Cette configuration particulière permet, avec un robot équipé de quatre de ces roues, de pouvoir se déplacer dans toutes les directions, avec une direction particulière (avant/arrière) ne générant pas de frottement supplémentaire à cause du roulement des roulettes internes, contrairement aux robots holonomes à trois roues, dont les roulettes génèrent un frottement supplémentaire quelle que soit la direction.
					</p>

					<figure style="text-align:center">
						<img src="/robot/Mecanum_wheel_control_principle.png" alt="mecanuum wheels" width="600" />
						<figcaption>Déplacements d'un robot équipé de quatre roues mecanum. Lorsque deux roues du même côté tournent en sens inverse, le déplacement vers l'avant ou l'arrière s'annule, tandis que le roulement des roulettes provoque un déplacement sur le côté. En jouant sur la vitesse des quatre roues, le robot peut se déplacer dans toutes les directions (<a href="https://en.wikipedia.org/wiki/Mecanum_wheel">source</a>).</figcaption>
					</figure>

				</section>

				
				<section class="listsection">

					<p>	
						Démonstration des possibilités de déplacement de la plateforme, ici contrôlé avec une manette de jeux :
					</p>
			
					<p style="text-align: center">
 						<video width="400" controls >
  							<source src="/robot/robot_control.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video> 
					</p>

					<p>	
						Code source :<br>
						- Code Arduino : <a href="/robot/robot_control_bluetooth.ino">robot_control_bluetooth.ino</a> <br>
						- Interface Java : <br>
						&emsp; Contrôle avec souris : <a href="/robot/robot_control_mouse.zip">robot_control_mouse.zip</a> (nécessite la librairie <a href="https://github.com/java-native/jssc/releases">JSSC</a>) <br>
						&emsp; Contrôle avec manette : <a href="/robot/robot_control_gamepad.zip">robot_control_gamepad.zip</a> (nécessite les librairies <a href="https://github.com/java-native/jssc/releases">JSSC</a> et <a href="https://jar-download.com/artifacts/net.java.jinput/jinput/">jinput</a>) <br>
					</p>

					<p>
						Instructions :<br>
				
						- Connecter le robot avec votre PC, en suivant le <a href="https://osoyoo.com/2022/07/05/v2-metal-chasiss-mecanum-wheel-robotic-for-arduino-mega2560-lesson-4-bluetooth-imitation-driving/">tutoriel d'Osoyoo</a> <br>
						- Téléverser le code .ino dans l'Arduino du robot<br>
						- Créer un nouveau projet Java, et importer les fichiers d'une des deux archives<br>
						- A l'aide de l'IDE Arduino, déterminer sur quel port le dongle Bluetooth est connecté : le programme envoie le mot "test" en boucle, que vous pouvez lire avec le moniteur série. Sélectionner ensuite le port permettant de se connecter à l'Arduino par USB pour libérer le port Bluetooth<br>
						- Dans la classe Main, indiquer le port utilisé par le dongle Bluetooth (ligne 6)<br>
						- Lancer le programme java. Une fenêtre doit s'ouvrir<br>
						- Cliquer dans la fenêtre pour faire apparaître un joystick virtuel. Sans relâcher le bouton, de déplacer le curseur autour du point initial pour contrôler le robot : bouton gauche pour avant/arrière/translations et bouton droit pour avant/arrière/rotations. Le robot s'arrête quand on relâche le bouton.<br>

						<figure style="text-align:center">
							<img src="/robot/virtual_joystick.png" alt="Joystick virtuel" width="250" />
						</figure>

						Pour utiliser la manette :<br>
						- Déterminer le nom du périphérique dans la liste qui s'affiche dans le terminal

						<figure style="text-align:center">
							<img src="/robot/omni_gamepad1.png" alt="liste périphériques" width="600" />
						</figure>

						- Copier le nom du périphérique dans la classe Main, ligne 7
						<figure style="text-align:center">
							<img src="/robot/omni_gamepad2.png" alt="Nom de la manette" width="600" />
						</figure>

						- Relancer l'application Java
					</p>
				</section>

				
				<section class="listsection">
			
					<p>	
						Intégration d'une centrale inertielle (IMU) GY-87 :
				
						<figure style="text-align:center">
							<img src="/robot/IMU.jpg" alt="IMU GY-87" width="250" />
							<figcaption>Centrale inertielle GY-87.</figcaption>
						</figure>
					</p>
			
					<p>		
						La centrale inertielle GY-87 est une carte de développement munie d'un accéléromètre et gyroscope MPU6050 permettant de mesurer les accélérations et les rotations sur 3 axes, d'un magnétomètre HMC5883L pouvant servir de boussole, et d'un capteur de pression barométrique BMP180. La carte peut être alimenté en 5V ou en 3.3V. Ces capteurs permettent une estimation relativement précise des déplacements du robot dans son environnement.
					</p>
			
					<p>
						La connexion de la carte IMU à l'Arduino est très simple :<br>
						- l'alimentation est reliée aux broches 5V et Gnd de l'Arduino. <br>
						- les données sont transmises par les broches SCL et SDA, connectées aux broches correspondantes sur l'Arduino (broches 'SCL 21' et 'SDA 20' de l'Arduino Mega du robot).
			
						<figure style="text-align:center">
							<img src="/robot/IMU_connect.png" alt="connexion de la carte IMU" width="600" />
							<figcaption>Connexion de la carte GY-87 à l'Arduino Mega du robot.</figcaption>
						</figure>
					</p>
			
					<p>
						Vous trouverez de nombreux détails pour intégrer cette carte IMU sur le site d'<a href="https://github.com/OlivierGeorgeon/osoyoo/">Olivier Georgeon</a>.
					</p>
							
					<p>
						La carte est maintenue dans un support imprimé en 3D et placée à l'avant du robot. Vous trouverez ci-dessous le modèle du support et les codes Arduino et java permettant de lire les informations issues de la carte.
					</p>
					
					<p>	
						Code source :<br>
						- Modèle support : <a href="/robot/IMU.stl">IMU.stl</a> <br>
						- Code Arduino : <a href="/robot/robot_control_bluetooth_IMU.ino">robot_control_bluetooth_IMU.ino</a> <br>
						- Interface Java : <br>
						&emsp; Contrôle avec souris : <a href="/robot/robot_control_mouse_IMU.zip">robot_control_mouse_IMU.zip</a> (nécessite la librairie <a href="https://github.com/java-native/jssc/releases">JSSC</a>) <br>
						&emsp; Contrôle avec manette : <a href="/robot/robot_control_gamepad_IMU.zip">robot_control_gamepad_IMU.zip</a> (nécessite les librairies <a href="https://github.com/java-native/jssc/releases">JSSC</a> et <a href="https://jar-download.com/artifacts/net.java.jinput/jinput/">jinput</a>) <br>
					</p>
			
				</section>
			</section>

		
			<section class="subsection">
				<section class="listsection">
					
					<p>
						<center style="text-align: center;font-size: xx-large;">Une plateforme pour l'étude de modèles de navigation</center><br /><br />
					</p>

					<p>
						Afin de rendre le robot autonome, tout en permettant du traitement d'image et la localisation, la plateforme a été équipée d'un nano-ordinateur. Les célèbres Raspberry Pi 4 étant en rupture de stock en cette période, notre choix s'est porté sur un modèle équivalent : le <a href="https://wiki.banana-pi.org/Banana_Pi_BPI-M5">Banana Pi M5</a>. Celui-ci n'étant pas équipé d'une connexion Wifi, un dongle doit être ajouté. Nous avons utilisé un dongle basé sur un chipset Ralink RT5370-MT7601 (attention, le chipset RT5370-MTK7601 ne fonctionne pas avec la version de Raspbian utilisée). La caméra binoculaire est une caméra de Playstation 4 (PS4Eye) modifiée pour pouvoir être branchée sur un port USB3 (tutoriel disponible <a href="https://www.instructables.com/HACK-PlayStation-4-Cam-Into-Cheap-3D-Depth-Camera-/">ici</a>). L'ensemble est alimenté par une batterie de type powerbank de 20Ah, indépendante de la batterie des moteurs. Nous utilisons un cable USB avec interrupteur pour ne pas avoir à débrancher le câble pour couper l'alimentation. Le système communique avec la plateforme robotique par une connexion USB avec l'Arduino du robot.<br/>
					</p>
		
					<figure style="text-align:center">
						<img src="/robot/platform_components.jpg" alt="Composants additionnels" width="600" />
						<figcaption>Composants additionnels de la plateforme. De gauche à droite : l'ordinateur de bord Banana Pi M5 (équipé d'un dongle Wifi), La caméra binoculaire PS4 eye modifiée, et la batterie Powerbank.</figcaption>
					</figure>
					<br/>

					<p>
						Les composants additionnels sont placés sur une plaque de plexi de 5mm d'épaisseur maintenue au châssis par des entretoises, et tenu par des supports imprimés en 3D. Le boîtier du Banana Pi est basé sur <a href="https://www.thingiverse.com/thing:5147307">ce modèle</a>. Le support de la batterie est conçue pour une powerbank modèle Intenso XS20000, de dimensions 128 x 70 x 25,2 mm. Ce support maintien également le dongle Wifi, relié au Banana Pi par une rallonge USB de 10cm. Les supports de la caméra sont conçus pour la version 1 de la PS4 eye. Il faut en imprimer deux, dont un qui doit être inversé (supports symétriques).<br />
						- Support batterie : <a href="/robot/batterie_support.stl">batterie_support.stl</a><br />
						- Support caméra : <a href="/robot/camera_support.stl">camera_support.stl</a><br />
					</p>
					<br />

					<figure style="text-align:center">
						<img src="/robot/architecture_robot.svg" alt="Architecture de la plateforme" width="500" />
						<figcaption>Architecture de la plateforme : à gauche, l'ordinateur de bord, équipé d'une caméra binoculaire et d'un dongle Wifi, et alimenté par une powerbank. Il communique par USB avec l'Arduino de la plateforme, qui pilote les moteurs via des cartes de contrôle. Les moteurs sont alimentés par leur propre batterie.</figcaption>
					</figure>
					<br />

					<figure style="text-align:center">
						<a href="/robot/platform_details.jpg"><img src="/robot/platform_details.jpg" style="border: 2px blue solid" alt="détails composants additionnels" width="500" /></a>
						<figcaption>Détails de la plaque supérieure supportant les composants additionnels.</figcaption>
					</figure>
					<br />

					<p>	
						D'un point de vue logiciel, le Banana Pi du robot tourne sous une version de Raspbian adaptée à cette carte (version 2023-05-03, avec un noyau 5.17.2-meson64) disponible sur ce <a href="https://wiki.banana-pi.org/Banana_Pi_BPI-M5#Raspbian">wiki</a>. Il est conseillé d'installer les logiciels suivants :<br/>
						- openssh-server : pour se connecter en ssh depuis un autre poste,<br/>
						- xrdp : pour se connecter à distance sur une session graphique (e.g. Remmina),<br/>
						- OpenJDK : pour utiliser le code fourni sur cette page. Le robot utilise la version 17,<br/>
						- Eclipse : une IDE pour développer en Java directement sur le robot,<br/>
						- Python3 : pour utiliser le programme qui transfère le firmware dans la caméra,<br/>
						- guvcview : pour tester la caméra,<br/>
						- libjssc-java : une librairie Java pour utilsier le port série.<br/>
					</p>
					
					<p>
						La communication s'effectue par le port série USB. Voici le code source pour l'Arduino et une classe Java servant d'interface :<br />
						- code arduino : <a href="/robot/robot_control_usb.ino">robot_control_usb.ino</a> <br>
						- interface Java : <a href="/robot/Robot.java">Robot.java</a> (nécessite la librairie <a href="https://github.com/java-native/jssc/releases">JSSC</a>, sur le robot, il faudra utiliser le .jar disponible dans /usr/share/java/ après avoir installé libjssc-java) <br>
					</p>

					<p>
						Il faut ensuite installer OpenCV pour Java. Vous pouvez suivre ce <a href="https://qengineering.eu/install-opencv-4.4-on-raspberry-pi-4.html">tutoriel</a>. Il faut toutefois s'assurer, après la commande <code>CMake</code>, que le compilateur a bien trouvé les exécutables ANT et JNI. Il faut également sauter la commande <code>make clean</code> pour ne pas supprimer le fichier .jar généré. Si tout s'est bien déroulé, après une longue compilation, le fichier .jar doit être présent dans le dossier /home/pi/opencv/build/bin, et les librairies .so dans le dossier /home/pi/opencv/build/lib.
					</p>
					
				</section>


				<section class="listsection">

					<p>
						Utilisation de la caméra PS4Eye :
					</p>

					<p>
						La PS4Eye necessite de transférer un firmware avant utilisation. Il faut dans un premier temps débrancher puis rebrancher la caméra. Il est possible de tester si la caméra est reconnue avec un <code>lsusb</code> dont le résultat doit faire apparaître la caméra :<br/>
						<br/>
					
						<code>&emsp;ID 05a9:0580 OmniVision Technologies</code> <br/>
						<br/>
					
						Dans le cas contraire, il faut vérifier les connexions.<br/><br/>
						On transfère ensuite le firmware à l'aide du code fourni <a href="https://github.com/sieuwe1/PS4-eye-camera-for-linux-with-python-and-OpenCV">ici</a> :<br/><br/>
					
						<code>
						&emsp;cd Firmware_loader<br/>
						&emsp;sudo python3 ps4eye_init.py<br/>
						</code><br/>
				
						Si tout se passe bien, le message suivant doit s'afficher :<br/><br/>
					
						<code>&emsp;PS4 camera firmware uploaded and device reset</code><br/><br/>
					
						La caméra peut ensuite être utilisée comme une simple webcam (ici, avec guvcview) :
					
						<figure style="text-align:center">
							<img src="/robot/camera_view1.jpg" alt="vue de la caméra" width="600" />
							<figcaption>Image issue de la caméra : la caméra fournie des images de 3748x808 pixels, comprenant les images des deux caméras et des versions compressées de ces images.</figcaption>
						</figure>
						<br/>
					
						Les images fournies par la caméra ont une taille de 3748x808 pixels, et comprennent les images issues des deux caméras, de taille 1280x800 pixels chacunes, ainsi que des versions réduites de ces images. Le code suivant permet d'obtenir deux images à partir du flux de la caméra :<br/>
					
						- <a href="/robot/camera.zip">camera.zip</a>
						<br/>
						<br/>
					
						Pour utiliser ce code, créez un nouveau projet (par exemple dans Eclipse), et importez les fichiers de l'archive ci-dessus. Associez la librairie opencv.jar (Build path->Configure build path->Libraries->add external JAR) obtenue après la compilation, puis spécifiez l'emplacement des librairies natives :<br/>
					
						<figure style="text-align:center">
							<img src="/robot/dependancies_opencv.png" alt="librairies OpenCV" width="300" />
						</figure>
						<br/>
					
						Dans la classe Main, il est possible de spécifier le numéro de la caméra, le framerate (si l'image est trop sombre, baisser à 30fps pour augmenter le temps d'exposition), et activer le réglage automatique de la luminosité (uniquement sous Linux).<br/>
						L'exécution du programme ouvre une fenêtre affichant les deux images gauche et droite séparées :
					
						<figure style="text-align:center">
							<img src="/robot/camera_view2.png" alt="deux images issues de la caméra" width="600" />
							<figcaption>Images gauche et droite séparées.</figcaption>
						</figure>
				
						A noter : ce programme fonctionne aussi sur un PC Linux ou Windows.
					</p>
							
				</section>
			</section>

			
			<section class="subsection">
				<section class="listsection">
				
					<p>
						<center style="text-align: center;font-size: xx-large;">Système de localisation pour une flottille de robots</center><br /><br />
					</p>

					<p>
						L'étude de nos modèles de navigation portent également sur la distribution du modèle sur plusieurs robots. Cette distribution doit permettre de répartir les besoins en ressources, et donc les ressources individuelles nécessaires pour chaque robot. Le principe implique que pendant qu'un robot guide le groupe, les autres peuvent être affectés à d'autres tâches, tout en suivant le robot-guide. Nous avons ainsi développé un système de suivi très simple mais efficace basé sur un code-barre vertical inscrit sur un tube en papier placé au sommet de chaque robot. Ce code-barre permet d'inscrire un code sur 4 bits et fourni une approximation de l'orientation (Voir figure ci-dessous).
					</p>
					<br/>
					
					<figure style="text-align:center">
						<img src="/robot/robot_barcode.jpg" alt="robot with barcode" width="300" />
						<figcaption>Le robot équipé de son code-barre vertical. Les deux lignes du haut et la ligne du bas constituent des marqueurs pour reconnaitre le code-barre. Quatre lignes peuvent ensuite figurer sur le code, fournissant un code à 4 bits (ici, le code 1001=9). La ligne de hauteur variable permet une approximation de l'orientation du robot, en comparant la hauteur de la ligne observée et la hauteur du code-barre.</figcaption>
					</figure>

					<p>
						Template du code-barre (doit être imprimé sur une page A4) : <a href="/robot/barcode.svg">barcode.svg</a>
					</p>
					<br/>
					
					<p>
						Exploration avec deux robots. Un robot, ici piloté à distance, cartographie l'environnement. Le second robot suit le premier, puis, à intervalles réguliers, échangent leurs positions. Chaque robot n'enregistre qu'une partie de la carte. Notre <a href="https://gaysimon.github.io/projects/navig_swarm1_en.html">modèle de navigation bio-inspiré</a> autorise cette distribution, mais aussi une exploitation d'un tel modèle distribué.
					</p>

					<p style="text-align: center">
 						<video width="300" controls >
  							<source src="/robot/multi_robots.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video> 
					</p>
					<br />
					
					<p>
						Code source du système de détection de code-barre :<br />
						- Classe java : <br />
					</p>
			
				</section>
			</section>

		</div>

		
		<footer>
			<p>
				Derniers ajouts par section
			</p>

			<div class="footsection">
				<p>
					&nbsp;ROBOTS :<br />
					&nbsp;&nbsp;<a href="index.php?page=john1">Johnny </a> <br />
					&nbsp;&nbsp;<a href="index.php?page=john2">Johnny 2.0 </a> <br />
					&nbsp;&nbsp;<a href="index.php?page=eirl">ErnestIRL </a> <br />
					&nbsp;&nbsp;<a href="index.php?page=psik">PsikHarpax </a> <br />
					&nbsp;&nbsp;<a href="index.php?page=ecce">EcceRobot </a> <br />
					&nbsp;&nbsp;<a href="index.php?page=epuck">ePuck </a> <br />
				</p>
			</div>

			<div class="footsection">
				<p>
					&nbsp;SOFTWARES :<br />
					&nbsp;&nbsp;<a href="index.html">SMA </a> <br />
					&nbsp;&nbsp;<a href="index.php?page=vacu">vacuumSG </a> <br />
					&nbsp;&nbsp;<a href="index.php?page=littleai">Java LittleAI </a> <br />
					&nbsp;&nbsp;<a href="index.php?page=mvac">Microvacuum </a> <br />
					&nbsp;&nbsp;<a href="index.php?page=esimu">ErnestIRL simulator </a> <br />
				</p>
			</div>

			<div class="footsection">
				<p>
					&nbsp;Le projet Ernest
				</p>
			</div>
		</footer>

	</body>
</html>
